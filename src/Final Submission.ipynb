{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solution.py\n",
    "\n",
    "# Import important libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from itertools import chain\n",
    "from itertools import repeat\n",
    "from collections import OrderedDict\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "config = {}\n",
    "\n",
    "# E.g. \"1.256660 0.431805 -4.981400\"\n",
    "def parse_coords(text):\n",
    "    return [float(x) for x in text.split(' ')]\n",
    "\n",
    "def iter_dataset(xml_tree):\n",
    "    for child in xml_tree.getroot():\n",
    "        name = int(child.tag.split('_')[1])\n",
    "        try:\n",
    "            energy =  float(child.find('energy').text)\n",
    "        except AttributeError:\n",
    "            energy = np.nan\n",
    "        atoms = [parse_coords(element.text) for element in child.find('coordinates').findall('c')]\n",
    "        for i, coords in enumerate(atoms):\n",
    "            yield {'Entry':name, 'Energy':energy, 'Atom': i, 'X':coords[0], 'Y':coords[1], 'Z':coords[2]}\n",
    "\n",
    "def parse_dataset(xml_file):\n",
    "    xml_tree = ET.parse(xml_file)\n",
    "    training_set = list(iter_dataset(xml_tree))\n",
    "\n",
    "    return pd.DataFrame(training_set, columns=('Entry', 'Energy', 'Atom', 'X', 'Y', 'Z'))\n",
    "    \n",
    "def get_pos(data, entry):\n",
    "    # Convert the X, Y, Z position for entry to a numpy array of size 60x3\n",
    "    \n",
    "    # Get single entry\n",
    "    E = data[data['Entry'] == entry]\n",
    "    if E.empty:\n",
    "        print('Invalid Entry id!')\n",
    "        return None\n",
    "    \n",
    "    # Get the position in format Nx3\n",
    "    E_ = E.apply(lambda row: [row['X'], row['Y'], row['Z']], axis=1).values\n",
    "    \n",
    "    # Transform it to a numpy array\n",
    "    Epos = np.reshape(list(chain(*E_)), (60, 3))\n",
    "    \n",
    "    return Epos\n",
    "\n",
    "\n",
    "def get_distance(pos0, pos1, method='atom_pos'):\n",
    "    # Calculate a distance value between e0 and e1 based on\n",
    "    # method='atom_pos' ... their cummulative difference in atom positions\n",
    "    # method='mesh_size' ... the abs. diff in mean atom gap size (i.e mesh size)\n",
    "    # method='mesh_size_variance' ... the abs. diff of variance of the mean atom gap size (i.e variance of the mesh size)\n",
    "    \n",
    "    if method == 'atom_pos':\n",
    "        # Calculate the distance matrix\n",
    "        D = cdist(pos0, pos1, metric='euclidean')\n",
    "\n",
    "        # Find the closest match for each point\n",
    "        assignment = np.argsort(D, axis=1)[:, 0]\n",
    "\n",
    "        # Calculate distance between each point to its assigned point\n",
    "        distance = np.sum(np.sqrt(np.sum((pos0 - pos1[assignment, :])**2, axis=1)))\n",
    "        \n",
    "    elif method == 'mesh_size':\n",
    "        # For each atom calculate the mean distance to its three closest neighbours\n",
    "        D0 = cdist(pos0, pos0, metric='euclidean')\n",
    "        D0.sort(axis=1)\n",
    "        D0_mesh_size = np.mean(D0[:, 1:4])\n",
    "\n",
    "        D1 = cdist(pos1, pos1, metric='euclidean')\n",
    "        D1.sort(axis=1)\n",
    "        D1_mesh_size = np.mean(D1[:, 1:4])\n",
    "        \n",
    "        distance = np.abs(D0_mesh_size - D1_mesh_size)\n",
    "\n",
    "    elif method == 'mesh_size_variance':\n",
    "        # For each atom calculate the mean distance to its three closest neighbours\n",
    "        D0 = cdist(pos0, pos0, metric='euclidean')\n",
    "        D0.sort(axis=1)\n",
    "        D0_mesh_size_var = np.var(np.mean(D0[:, 1:4], axis=1))\n",
    "\n",
    "        D1 = cdist(pos1, pos1, metric='euclidean')\n",
    "        D1.sort(axis=1)\n",
    "        D1_mesh_size_var = np.var(np.mean(D1[:, 1:4], axis=1))\n",
    "        \n",
    "        distance = np.abs(D0_mesh_size_var - D1_mesh_size_var)\n",
    "       \n",
    "    return distance\n",
    "\n",
    "\n",
    "def calculate_ranking(prediction_data, lookup_data, distance_method = ''):\n",
    "    # For each entry in 'prediction_data' rank all entries in 'data'\n",
    "    #\n",
    "    # Return a ordered Dictionary containg for each prediction_data Entry\n",
    "    # a tuple describing the similary/distance to each entry in the lookup table.\n",
    "    \n",
    "    prediction_entries = prediction_data['Entry'].drop_duplicates()\n",
    "    lookup_entries = lookup_data['Entry'].drop_duplicates()\n",
    "    \n",
    "    results = OrderedDict()\n",
    "    for pre in prediction_entries:\n",
    "        ranking = []\n",
    "        e0pos = get_pos(prediction_data, pre)\n",
    "        for (e0, e1) in zip(repeat(pre), lookup_entries):\n",
    "            e1pos = get_pos(lookup_data, e1)\n",
    "            d = get_distance(e1pos, e0pos, method=distance_method)\n",
    "            ranking.append((d, e1))\n",
    "\n",
    "        ranking.sort()\n",
    "        results[pre] = ranking\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results, lookup_data):\n",
    "    # Based on the ranking calculate a energy value for each entry by\n",
    "    # taking the mean energy value of its 3 closest matches.\n",
    "\n",
    "    entries = []\n",
    "    predictions = []\n",
    "    for entry_id in results.keys():\n",
    "        entries.append(entry_id)\n",
    "        closest_entries = [res[1] for res in results[entry_id][0:3]]\n",
    "        predictions.append(np.mean(get_energies(lookup_data, closest_entries)))\n",
    "    \n",
    "    return entries, predictions\n",
    "\n",
    "\n",
    "def single_stage_prediction(training, validation):\n",
    "    ranking = calculate_ranking(validation, training, distance_method='atom_pos')   \n",
    "    entries, predictions = get_predictions(ranking, training)\n",
    "    return entries, predictions\n",
    "\n",
    "\n",
    "def two_stage_prediction(training, validation, energy_sw=0.05, distance_methods=['atom_pos', 'mesh_size_variance']):\n",
    "    ranking = calculate_ranking(validation, training, distance_method=distance_methods[0])   \n",
    "    entries, predictions = get_predictions(ranking, training)\n",
    "\n",
    "    # For each entry in the first prediction generate a subset of the training data\n",
    "    # and apply another distance metric to the subset in order to calculate\n",
    "    # a improved prediction\n",
    "    new_predictions = []\n",
    "    for entry_id, predicted_energy in zip(entries, predictions):\n",
    "        # Calculate a subset of the data\n",
    "        training_subset = training[(training['Energy'] > (predicted_energy-energy_sw)) & (training['Energy'] < (predicted_energy+energy_sw))]\n",
    "        validation_subset = validation[validation['Entry'] == entry_id]\n",
    "\n",
    "        new_ranking = calculate_ranking(validation_subset, training_subset, distance_method=distance_methods[1])   \n",
    "        _, new_prediction = get_predictions(new_ranking, training_subset)\n",
    "    \n",
    "        new_predictions.append(new_prediction[0])\n",
    "    \n",
    "    return entries, new_predictions\n",
    "\n",
    "\n",
    "############### HELPER FUNCTIONS - NOT PART OF THE ALGORITHM ###############\n",
    "\n",
    "def evaluate_prediction(entry_ids, predicted_energies, lookup_table):\n",
    "    # Calculate the prediction error\n",
    "    prediction_errors = []\n",
    "    for entry_id, predicted_energy in zip(entry_ids, predicted_energies):\n",
    "        real_energy = lookup_table[lookup_table['Entry'] == entry_id]['Energy'].values[0]\n",
    "        prediction_errors.append(predicted_energy - real_energy)\n",
    "        \n",
    "    return np.array(prediction_errors)\n",
    "\n",
    "\n",
    "def cross_validation(n_tests, n_entries, training_data, prediction_function, kwargs={}):\n",
    "    prediction_errors = np.zeros(shape=(n_tests, n_entries))    \n",
    "    for n in range(0, n_tests):\n",
    "        # Split the training data into a new set of training and validation data in order to test the algorithm\n",
    "        validation_entries = set(np.random.choice(training_data['Entry'].unique(), n_entries, replace=False))\n",
    "        training_entries = set(training_data['Entry'].unique()) - validation_entries\n",
    "        \n",
    "        print('Running Test (%d/%d) with validation entries %s ...' % (n+1, n_tests, validation_entries))\n",
    "        \n",
    "        training = training_data[training_data['Entry'].isin(training_entries)]\n",
    "        validation = training_data[training_data['Entry'].isin(validation_entries)]\n",
    "\n",
    "        entries, predictions = prediction_function(training, validation, **kwargs)\n",
    "        \n",
    "        prediction_errors[n, :] = evaluate_prediction(entries, predictions, training_data)\n",
    "    \n",
    "    return prediction_errors\n",
    "\n",
    "\n",
    "def get_energies(table, entries):\n",
    "    return [table[table['Entry'] == entry]['Energy'].values[0] for entry in entries]\n",
    "        \n",
    "def get_closest_entries(table, energy):\n",
    "    uT = table[['Entry', 'Energy']].drop_duplicates()    \n",
    "    energies = uT['Energy'].values\n",
    "    entries = uT['Entry'].values    \n",
    "        \n",
    "    diff_energies = (energies - energy)**2\n",
    "    closest_energies = np.argsort(diff_energies)\n",
    "    closest_entries = entries[closest_energies]\n",
    "    \n",
    "    return closest_entries, energies[closest_energies]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "training = parse_dataset('data/new_training_set.xml')\n",
    "validation = parse_dataset('data/new_validation_set.xml')\n",
    "submission = pd.read_csv('data/return_file_template.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform prediction\n",
    "entries, energies = two_stage_prediction(training, validation, energy_sw=0.5, distance_methods=['atom_pos', 'mesh_size_variance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write submission file based on template\n",
    "submission['energy'] = energies\n",
    "submission.to_csv('final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_n</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entry_2</td>\n",
       "      <td>-0.396967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entry_16</td>\n",
       "      <td>-0.389867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entry_28</td>\n",
       "      <td>-0.199567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entry_40</td>\n",
       "      <td>-0.393767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entry_44</td>\n",
       "      <td>-0.173800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>entry_49</td>\n",
       "      <td>-0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>entry_61</td>\n",
       "      <td>-0.546433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>entry_68</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>entry_74</td>\n",
       "      <td>-0.314733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entry_75</td>\n",
       "      <td>-0.260933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>entry_90</td>\n",
       "      <td>-0.356333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>entry_126</td>\n",
       "      <td>-0.001733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>entry_132</td>\n",
       "      <td>-0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>entry_141</td>\n",
       "      <td>-0.177533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>entry_157</td>\n",
       "      <td>-0.221867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>entry_168</td>\n",
       "      <td>-0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>entry_169</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entry_176</td>\n",
       "      <td>-0.371367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>entry_191</td>\n",
       "      <td>-0.230233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>entry_198</td>\n",
       "      <td>-0.214533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>entry_202</td>\n",
       "      <td>-0.224467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>entry_205</td>\n",
       "      <td>-0.287800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      entry_n    energy\n",
       "0     entry_2 -0.396967\n",
       "1    entry_16 -0.389867\n",
       "2    entry_28 -0.199567\n",
       "3    entry_40 -0.393767\n",
       "4    entry_44 -0.173800\n",
       "5    entry_49 -0.004700\n",
       "6    entry_61 -0.546433\n",
       "7    entry_68  0.003400\n",
       "8    entry_74 -0.314733\n",
       "9    entry_75 -0.260933\n",
       "10   entry_90 -0.356333\n",
       "11  entry_126 -0.001733\n",
       "12  entry_132 -0.191800\n",
       "13  entry_141 -0.177533\n",
       "14  entry_157 -0.221867\n",
       "15  entry_168 -0.015900\n",
       "16  entry_169  0.002400\n",
       "17  entry_176 -0.371367\n",
       "18  entry_191 -0.230233\n",
       "19  entry_198 -0.214533\n",
       "20  entry_202 -0.224467\n",
       "21  entry_205 -0.287800"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
